{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kcy49t-KxPaq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2655,
     "status": "ok",
     "timestamp": 1646630849783,
     "user": {
      "displayName": "Andreas Aditya Alvaro Harryanto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12287744254258336802"
     },
     "user_tz": -420
    },
    "id": "Kcy49t-KxPaq",
    "outputId": "33a76871-a676-4f21-bc9f-e8d0fe6960da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbdd79",
   "metadata": {
    "id": "88cbdd79"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4218637",
   "metadata": {
    "id": "e4218637"
   },
   "outputs": [],
   "source": [
    "music_path = r'/content/drive/MyDrive/Thesis/Datasets/GTZAN Dataset/genres_original'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w-kafU3qhDi1",
   "metadata": {
    "id": "w-kafU3qhDi1"
   },
   "source": [
    "n_mfcc \t= number of MFCCs to return<br>\n",
    "n_fft \t\t= length of FFT (Fast Fourier Transform) Window (default 2048)<br>\n",
    "hop_length\t= samples between successive frames (default 512)<br>\n",
    "ff_dim \t\t= feed forward dimension (output Conv1D)<br>\n",
    "mlp_units \t= output dense layer in mlp units  (array for loop multiple layer)<br>\n",
    "mlp_dropout \t= dropout rate for dropout layer in mlp units<br>\n",
    "batch_size \t= 16 (input length 31955 / batch_size 16 = 1997.1 -> ceil = 1998 di fit progress bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d006d9",
   "metadata": {
    "id": "07d006d9"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "\n",
    "num_segments = 10\n",
    "hop_length = 512\n",
    "sample_ps = int(SAMPLES_PER_TRACK/num_segments)\n",
    "expected_vects_ps = math.ceil(sample_ps/hop_length)\n",
    "n_fft = 2048\n",
    "n_mfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a7d8b",
   "metadata": {
    "id": "6c6a7d8b"
   },
   "outputs": [],
   "source": [
    "def add_noise(data, noise_factor):\n",
    "    noise = np.random.randn(len(data.reshape(-1, 1))).reshape(data.shape)\n",
    "    add_noise_data = data + noise_factor * noise\n",
    "    # Cast back to same data type\n",
    "    add_noise_data = add_noise_data.astype(type(data[0]))\n",
    "    return add_noise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510d7f9",
   "metadata": {
    "id": "5510d7f9"
   },
   "outputs": [],
   "source": [
    "def shift_time(data, sampling_rate, shift_max):\n",
    "    shift = np.random.randint(sampling_rate * shift_max)\n",
    "\n",
    "    direction = np.random.randint(0, 2)\n",
    "    if direction == 1:\n",
    "        shift = -shift\n",
    "        \n",
    "    shifted_data = np.roll(data, shift)\n",
    "    # Set to silence for heading/ tailing\n",
    "    if shift > 0:\n",
    "        shifted_data[:shift] = 0\n",
    "    else:\n",
    "        shifted_data[shift:] = 0\n",
    "    return shifted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ccab1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1927400,
     "status": "ok",
     "timestamp": 1646632777179,
     "user": {
      "displayName": "Andreas Aditya Alvaro Harryanto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12287744254258336802"
     },
     "user_tz": -420
    },
    "id": "b7ccab1d",
    "outputId": "ce59f28b-943f-45c0-ce67-2b555c1113b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : 0 blues\n",
      "Processing : 1 country\n",
      "Skipped : 1 country.00007.wav, len mfcc : 129, expected : 130\n",
      "Processing : 2 disco\n",
      "Skipped : 2 disco.00014.wav, len mfcc : 129, expected : 130\n",
      "Processing : 3 rock\n",
      "Processing : 4 hiphop\n",
      "Skipped : 4 hiphop.00032.wav, len mfcc : 127, expected : 130\n",
      "Processing : 5 pop\n",
      "Processing : 6 classical\n",
      "Skipped : 6 classical.00051.wav, len mfcc : 129, expected : 130\n",
      "Processing : 7 reggae\n",
      "Processing : 8 jazz\n",
      "Processing : 9 metal\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"mapping\": [],\n",
    "    \"mfcc\"   : [],\n",
    "    \"targets\" : [],\n",
    "}\n",
    "\n",
    "for i, (dirpath, dirnames, filenames) in enumerate(os.walk(music_path)):\n",
    "    if dirpath is not music_path:\n",
    "        dirpath_comp = dirpath.split(\"/\")\n",
    "        semantic_label = dirpath_comp[-1]\n",
    "        data[\"mapping\"].append(semantic_label)\n",
    "\n",
    "        print(f\"Processing : {i-1} {semantic_label}\")\n",
    "\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            signal,sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "            \n",
    "            noise_signal = add_noise(signal, 0.1)\n",
    "            shift_signal = shift_time(signal, SAMPLE_RATE, 1)\n",
    "            pitch_signal = librosa.effects.pitch_shift(signal, sr=SAMPLE_RATE, n_steps=4)\n",
    "            \n",
    "            for s in range(num_segments):\n",
    "                start_sample = sample_ps * s\n",
    "                finish_sample = start_sample + sample_ps\n",
    "                \n",
    "                mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],\n",
    "                                            sr = sr,\n",
    "                                            n_fft = n_fft,\n",
    "                                            n_mfcc = n_mfcc,\n",
    "                                            hop_length = hop_length).T\n",
    "                noise_mfcc = librosa.feature.mfcc(noise_signal[start_sample:finish_sample],\n",
    "                                            sr = sr,\n",
    "                                            n_fft = n_fft,\n",
    "                                            n_mfcc = n_mfcc,\n",
    "                                            hop_length = hop_length).T\n",
    "                shift_mfcc = librosa.feature.mfcc(shift_signal[start_sample:finish_sample],\n",
    "                                            sr = sr,\n",
    "                                            n_fft = n_fft,\n",
    "                                            n_mfcc = n_mfcc,\n",
    "                                            hop_length = hop_length).T\n",
    "                pitch_mfcc = librosa.feature.mfcc(pitch_signal[start_sample:finish_sample],\n",
    "                                            sr = sr,\n",
    "                                            n_fft = n_fft,\n",
    "                                            n_mfcc = n_mfcc,\n",
    "                                            hop_length = hop_length).T\n",
    "\n",
    "                if len(mfcc)==expected_vects_ps:\n",
    "                    data[\"mfcc\"].append(mfcc.tolist())\n",
    "                    data[\"targets\"].append(i-1)\n",
    "                    \n",
    "                    data[\"mfcc\"].append(noise_mfcc.tolist())\n",
    "                    data[\"targets\"].append(i-1)\n",
    "                    \n",
    "                    data[\"mfcc\"].append(shift_mfcc.tolist())\n",
    "                    data[\"targets\"].append(i-1)\n",
    "                    \n",
    "                    data[\"mfcc\"].append(pitch_mfcc.tolist())\n",
    "                    data[\"targets\"].append(i-1)\n",
    "                else:\n",
    "                    print(f\"Skipped : {i-1} {f}, len mfcc : {len(mfcc)}, expected : {expected_vects_ps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0c748",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1646632777180,
     "user": {
      "displayName": "Andreas Aditya Alvaro Harryanto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12287744254258336802"
     },
     "user_tz": -420
    },
    "id": "18c0c748",
    "outputId": "0b8a12a1-0683-471d-e41a-60ee04a4f0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39944\n",
      "39944\n"
     ]
    }
   ],
   "source": [
    "print(len(data['mfcc']))\n",
    "print(len(data['targets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60221ee",
   "metadata": {
    "id": "e60221ee"
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5c17f",
   "metadata": {
    "id": "c8a5c17f"
   },
   "outputs": [],
   "source": [
    "hf = h5py.File(r'/content/drive/MyDrive/Thesis/Datasets/music_dataset_augmented.h5', 'w')\n",
    "hf.create_dataset('inputs', data=data['mfcc'])\n",
    "hf.create_dataset('targets', data=data['targets'])\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D5v8XTnLwg0t",
   "metadata": {
    "id": "D5v8XTnLwg0t"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/drive/MyDrive/Thesis/Datasets/data_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(data['mapping'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2zgOdC4ZhOC4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1646632831696,
     "user": {
      "displayName": "Andreas Aditya Alvaro Harryanto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12287744254258336802"
     },
     "user_tz": -420
    },
    "id": "2zgOdC4ZhOC4",
    "outputId": "cdf97104-ef3b-45fd-a4e5-89b282da1b4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blues',\n",
       " 'country',\n",
       " 'disco',\n",
       " 'rock',\n",
       " 'hiphop',\n",
       " 'pop',\n",
       " 'classical',\n",
       " 'reggae',\n",
       " 'jazz',\n",
       " 'metal']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/content/drive/MyDrive/Thesis/Datasets/data_mapping.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n",
    "\n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8e57f",
   "metadata": {
    "id": "9ef8e57f"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "    hf = h5py.File(dataset_path, 'r')\n",
    "    inputs = hf.get('inputs')\n",
    "    targets = hf.get('targets')\n",
    "    \n",
    "    inputs = np.array(inputs)\n",
    "    targets = np.array(targets)\n",
    "    hf.close()\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f704157",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6610,
     "status": "ok",
     "timestamp": 1646632838303,
     "user": {
      "displayName": "Andreas Aditya Alvaro Harryanto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12287744254258336802"
     },
     "user_tz": -420
    },
    "id": "0f704157",
    "outputId": "3747354b-5de7-422b-f869-8c3dc04312bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9986, 130, 13) (9986,)\n",
      "(19972, 130, 13) (19972,)\n",
      "(19972, 130, 13) (19972,)\n",
      "(19972, 130, 13) (19972,)\n",
      "(39944, 130, 13) (39944,)\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = load_data(r'/content/drive/MyDrive/Thesis/Datasets/music_dataset_augmented.h5')\n",
    "print(inputs.shape, targets.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Preprocessing + Augmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
